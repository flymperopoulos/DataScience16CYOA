{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emily Wang and Filippos Lymperopoulos | Data Science 2016 | CYOA: sfcrime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 17, 2016\n",
    "\n",
    "### Goals for *model_iter_7_emily.ipynb*\n",
    "\n",
    "* Improve readability\n",
    "* Several different folds; summary statistics on k folds\n",
    "* Different ideas for feature engineering/encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 15, 2016\n",
    "\n",
    "### The process\n",
    "\n",
    "* Import libraries and training data\n",
    "* Feature engineering / preprocessing: \n",
    "    * Make \"useful\" combinations of features to give our model; \n",
    "    * Also encode categorical things in an intelligent way; \n",
    "    * Can choose to only use a subset of features if desired\n",
    "* Partition your data (cross-validation kfolds, etc)\n",
    "* Model fit\n",
    "* Make some predictions\n",
    "* Compute the logloss score\n",
    "* Reflect; iterate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's import some useful libraries and import the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>WARRANT ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-13 23:33:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>VANNESS AV / GREENWICH ST</td>\n",
       "      <td>-122.424363</td>\n",
       "      <td>37.800414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dates        Category                  Descript  DayOfWeek  \\\n",
       "0 2015-05-13 23:53:00        WARRANTS            WARRANT ARREST  Wednesday   \n",
       "1 2015-05-13 23:53:00  OTHER OFFENSES  TRAFFIC VIOLATION ARREST  Wednesday   \n",
       "2 2015-05-13 23:33:00  OTHER OFFENSES  TRAFFIC VIOLATION ARREST  Wednesday   \n",
       "\n",
       "  PdDistrict      Resolution                    Address           X          Y  \n",
       "0   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST -122.425892  37.774599  \n",
       "1   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST -122.425892  37.774599  \n",
       "2   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST -122.424363  37.800414  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "\n",
    "# Convert the Dates column of our provided data from string to datetime format.\n",
    "train = pd.read_csv('train.csv', parse_dates = ['Dates'])\n",
    "test = pd.read_csv('test.csv', parse_dates = ['Dates'])\n",
    "\n",
    "# Print the first 3 rows of the dataframe.\n",
    "display(train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering, Preprocessing\n",
    "\n",
    "Make a class that will:\n",
    "* Extract the time features we want to use for the model (e.g. year, season, month, day, etc.)\n",
    "* Encode categorical variables in a meaningful way: contains a preprocessor that can both transform and inverse_transform the categorical variables\n",
    "* Return a transformed dataframe to be given to the model\n",
    "* Maybe: allow for some flexibility with what is in the transformed dataframe (to iterate quickly) (e.g. choosing how many time features you want in this experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SFP = SFCrime Preprocessor\n",
    "class SFP():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.Y_encoder = preprocessing.LabelEncoder()\n",
    "    \n",
    "    # Prepare inputs\n",
    "    def prep_district(self):\n",
    "        # one hot encoding\n",
    "        return pd.get_dummies(self.data.PdDistrict)\n",
    "    \n",
    "    def prep_hour(self):\n",
    "        # a continuous value from 0 to 23\n",
    "        return self.data.Dates.dt.hour # Gets the hour portion form the \"Dates\" column\n",
    "    \n",
    "    def prep_day(self):\n",
    "        # one hot encoding\n",
    "        return pd.get_dummies(self.data.DayOfWeek)\n",
    "    \n",
    "    def prep_years(self):\n",
    "        # beware: 2015 has significantly less incidents than the other years in this dataset.        \n",
    "        pass\n",
    "    \n",
    "    def concat_features(self):\n",
    "        hour = self.prep_hour()\n",
    "        day = self.prep_day()\n",
    "        district = self.prep_district()\n",
    "        return pd.concat([hour, day, district], axis=1)\n",
    "    \n",
    "    # Encode or decode classes\n",
    "    def encode_Y(self, Y):\n",
    "        return self.Y_encoder.fit_transform(Y)\n",
    "\n",
    "    def decode_Y(self, encoded_Y):\n",
    "        return self.Y_encoder.inverse_transform(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sfp = SFP(train)\n",
    "X = sfp.concat_features()\n",
    "y = sfp.encode_Y(train.Category)\n",
    "X.rename(columns = {'Dates':'Hour'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check\n",
    "\n",
    "Print out the head for X to ensure that our preprocessing via the SFP class worked as expected. \n",
    "We should have a number between 0 and 39 for Hour. Each incident should have a 1 in one of the weekday columns and 0 in all other weekday columns. Each incident should have a 1 in one of the PdDistrict columns and 0 ina ll other PdDistrict columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>BAYVIEW</th>\n",
       "      <th>CENTRAL</th>\n",
       "      <th>INGLESIDE</th>\n",
       "      <th>MISSION</th>\n",
       "      <th>NORTHERN</th>\n",
       "      <th>PARK</th>\n",
       "      <th>RICHMOND</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>TARAVAL</th>\n",
       "      <th>TENDERLOIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hour  Friday  Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  \\\n",
       "0    23       0       0         0       0         0        0          1   \n",
       "1    23       0       0         0       0         0        0          1   \n",
       "2    23       0       0         0       0         0        0          1   \n",
       "3    23       0       0         0       0         0        0          1   \n",
       "4    23       0       0         0       0         0        0          1   \n",
       "\n",
       "   BAYVIEW  CENTRAL  INGLESIDE  MISSION  NORTHERN  PARK  RICHMOND  SOUTHERN  \\\n",
       "0        0        0          0        0         1     0         0         0   \n",
       "1        0        0          0        0         1     0         0         0   \n",
       "2        0        0          0        0         1     0         0         0   \n",
       "3        0        0          0        0         1     0         0         0   \n",
       "4        0        0          0        0         0     1         0         0   \n",
       "\n",
       "   TARAVAL  TENDERLOIN  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note: this X and Y in particular are from the data in train.csv. See previous section for details.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the training data to the algorithm\n",
    "\n",
    "Decision trees are known to be good at handling categorical data. Let's try using some of the decision tree variations in scikit learn (decision tree, random forest, gradient boost, etc) and tweak some hyperparameters. We might even do some ensemble learning. Oooh shiny!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "_ = dtc.fit(X_train, y_train)\n",
    "y_predictions = dtc.predict_proba(X_test)\n",
    "dtc_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.007115993440352"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_log_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filippos says he thinks this log loss value of 3.007115993440352 is very deece. Confirmed by looking at the kaggle leaderboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A baseline: Predict all unseen incidents as LARCENY/THEFT\n",
    "\n",
    "So... we know from the exploration phase that the dataset is not evenly distributed (i.e. not all 39 classes are equally represented in the dataset. What's the log loss for a model that just predicts that all new data is the most common crime type ('LARCENCY/THEFT') ? This means that the baseline model outputs the same prediction each time: a 39 dimensional vector of 0s and a 1 in the corresponding column for LARCENY/THEFT. The 1 signifies 100% confidence that the unseen incident is LARCENY/THEFT. \n",
    "\n",
    "Another idea for a baseline would be to randomly guess an intger between 0 and 38 for each unseen incident. \n",
    "\n",
    "Since log loss is a new error metric for us amd we don't already have an intuition for what log loss values are possible/reasonable for this project, this baseline provides a sanity check to ensure that we should be getting a better score on the models we are trying to train well with feature engineering, different classifiers, ensembling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded = sfp.decode_Y(dtc.classes_)\n",
    "\n",
    "for x in range(len(decoded)):\n",
    "    if decoded[x] == \"LARCENY/THEFT\": \n",
    "        # we know from the exploration phase that LARCENY/THEFT was the most popular crime in train.csv\n",
    "        popular_index = x\n",
    "\n",
    "baseline_y_pred = np.zeros((y_test.shape[0], 39))\n",
    "baseline_y_pred[:,16] = 1\n",
    "\n",
    "baseline_log_loss = log_loss(y_test, baseline_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.658162800015461"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Using some hyperparameter values from DataQuest mission 75\n",
    "rf = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=4, min_samples_leaf=1) \n",
    "_ = rf.fit(X_train, y_train)\n",
    "y_predictions = rf.predict_proba(X_test)\n",
    "rf_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0111558103212417"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "[scikit learn cheatsheet advises us to look into SGD classifiers!](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "\n",
    "Understanding SGD:\n",
    "* yay Andrew Ng ML video\n",
    "* batch gradient descent (looks at all of the training examples in every iteration)\n",
    "* stochastic gradient descent (looks at only one training example in every iteration)\n",
    "    * how well is my hypothesis doing on a single example? for a given theta and x,y pair\n",
    "* different in the implementation details and making progress towards the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "_ = sgd.fit(X_train, y_train)\n",
    "y_predictions = sgd.predict_proba(X_test)\n",
    "sgd_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8818288538852848"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ooooooooooohhhh\"  -- Emily and Filippos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb = BernoulliNB()\n",
    "_ = nb.fit(X_train, y_train)\n",
    "y_nb_predictions = nb.predict_proba(X_test)\n",
    "nb_log_loss = log_loss(y_test, y_nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.609770164645326"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ooooooooooohhhh\" (again)  -- Emily and Filippos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps\n",
    "\n",
    "ASAP:\n",
    "* ~~prepare a submission to kaggle~~\n",
    "* Visualization of performance of the different models (include an ensemble learning result in here too)\n",
    "* ~~Comparison of log loss to the \"predict the most common thing\" baseline model (predict all unseen incidents to be LARCENY/THEFT)~~\n",
    "* *10 fold validation + mean and standard deviation for the metrics* - Emily\n",
    "* ~~log loss on each separate class (turn the multi class problem into 39 binary problems)~~\n",
    "    * --> discover more specifically which things the model predicts well and not so well \n",
    "    * --> more exploration and feature engineering in hopes of resolving the difference in performance between crime classes\n",
    "\n",
    "\n",
    "Backlog:\n",
    "* ~~explanation on SGD black box~~\n",
    "* ~~How to translate the 39-element outputs into more \"human readable\" outputs: TOP5~~\n",
    "* *Creative approaches to ensemble learning!* - Filippos \n",
    "* ~~Naive Bayes~~\n",
    "* Try playing with hyperparameters; see how changes in those values impact the logloss, and plot them (hyperparameter value vs. log loss)\n",
    "\n",
    "Process comments:\n",
    "* Feb 15, 2016: We're relatively happy with our current preprocessor to pause on the feature engineering and do experiments with the predictive models; we'll cycle back to the feature engineering if there's time and interest. :)\n",
    "* Feb 17, 2016: Now that we have the per class log loss values, we have some motivation to go back to the exploration phase to better understand the classes which the model had more trouble with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 16, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it out on the test set\n",
    "\n",
    "Preparing our inputs (the X matrix): This should all look very familiar to the cells in the previous sections.\n",
    "\n",
    "Be careful of your variable names!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>BAYVIEW</th>\n",
       "      <th>CENTRAL</th>\n",
       "      <th>INGLESIDE</th>\n",
       "      <th>MISSION</th>\n",
       "      <th>NORTHERN</th>\n",
       "      <th>PARK</th>\n",
       "      <th>RICHMOND</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>TARAVAL</th>\n",
       "      <th>TENDERLOIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hour  Friday  Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  \\\n",
       "0    23       0       0         0       0         0        0          1   \n",
       "1    23       0       0         0       0         0        0          1   \n",
       "2    23       0       0         0       0         0        0          1   \n",
       "3    23       0       0         0       0         0        0          1   \n",
       "4    23       0       0         0       0         0        0          1   \n",
       "\n",
       "   BAYVIEW  CENTRAL  INGLESIDE  MISSION  NORTHERN  PARK  RICHMOND  SOUTHERN  \\\n",
       "0        0        0          0        0         1     0         0         0   \n",
       "1        0        0          0        0         1     0         0         0   \n",
       "2        0        0          0        0         1     0         0         0   \n",
       "3        0        0          0        0         1     0         0         0   \n",
       "4        0        0          0        0         0     1         0         0   \n",
       "\n",
       "   TARAVAL  TENDERLOIN  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>BAYVIEW</th>\n",
       "      <th>CENTRAL</th>\n",
       "      <th>INGLESIDE</th>\n",
       "      <th>MISSION</th>\n",
       "      <th>NORTHERN</th>\n",
       "      <th>PARK</th>\n",
       "      <th>RICHMOND</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>TARAVAL</th>\n",
       "      <th>TENDERLOIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dates  Friday  Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  \\\n",
       "0     23       0       0         0       1         0        0          0   \n",
       "1     23       0       0         0       1         0        0          0   \n",
       "2     23       0       0         0       1         0        0          0   \n",
       "3     23       0       0         0       1         0        0          0   \n",
       "4     23       0       0         0       1         0        0          0   \n",
       "\n",
       "   BAYVIEW  CENTRAL  INGLESIDE  MISSION  NORTHERN  PARK  RICHMOND  SOUTHERN  \\\n",
       "0        1        0          0        0         0     0         0         0   \n",
       "1        1        0          0        0         0     0         0         0   \n",
       "2        0        0          0        0         1     0         0         0   \n",
       "3        0        0          1        0         0     0         0         0   \n",
       "4        0        0          1        0         0     0         0         0   \n",
       "\n",
       "   TARAVAL  TENDERLOIN  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sfp_submission will be used to preprocess the data from test.csv\n",
    "sfp_submission = SFP(test) \n",
    "X_submission = sfp_submission.concat_features() \n",
    "\n",
    "# Sanity check: These should not be the same, because X is from train.csv and X_submission is from test.csv\n",
    "display(X.head())\n",
    "display(X_submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As a first pass, using the sgd we trained earlier in the notebook\n",
    "y_predictions_submission = sgd.predict_proba(X_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.014604</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.006099</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>1.294395e-07</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.026601</td>\n",
       "      <td>0.012581</td>\n",
       "      <td>0.016840</td>\n",
       "      <td>0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.014604</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.006099</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>1.294395e-07</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.026601</td>\n",
       "      <td>0.012581</td>\n",
       "      <td>0.016840</td>\n",
       "      <td>0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.007507</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.008535</td>\n",
       "      <td>1.170716e-07</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.018756</td>\n",
       "      <td>0.008613</td>\n",
       "      <td>0.014536</td>\n",
       "      <td>0.000512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.013436</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.011383</td>\n",
       "      <td>1.262481e-07</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.029864</td>\n",
       "      <td>0.018258</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>0.001031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.013436</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.011383</td>\n",
       "      <td>1.262481e-07</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.029864</td>\n",
       "      <td>0.018258</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>0.001031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000329  0.014604  0.000042  0.000232  0.006099  0.000768  0.000461   \n",
       "1  0.000329  0.014604  0.000042  0.000232  0.006099  0.000768  0.000461   \n",
       "2  0.000117  0.008188  0.000041  0.000139  0.007507  0.000938  0.000440   \n",
       "3  0.000181  0.013436  0.000043  0.000203  0.005856  0.000689  0.000478   \n",
       "4  0.000181  0.013436  0.000043  0.000203  0.005856  0.000689  0.000478   \n",
       "\n",
       "         7         8         9     ...           29        30        31  \\\n",
       "0  0.005324  0.000957  0.000206    ...     0.000178  0.000886  0.001020   \n",
       "1  0.005324  0.000957  0.000206    ...     0.000178  0.000886  0.001020   \n",
       "2  0.004081  0.001054  0.000210    ...     0.000139  0.001140  0.000989   \n",
       "3  0.003131  0.000869  0.000214    ...     0.000184  0.000910  0.001094   \n",
       "4  0.003131  0.000869  0.000214    ...     0.000184  0.000910  0.001094   \n",
       "\n",
       "         32            33        34        35        36        37        38  \n",
       "0  0.012760  1.294395e-07  0.002471  0.026601  0.012581  0.016840  0.001507  \n",
       "1  0.012760  1.294395e-07  0.002471  0.026601  0.012581  0.016840  0.001507  \n",
       "2  0.008535  1.170716e-07  0.002079  0.018756  0.008613  0.014536  0.000512  \n",
       "3  0.011383  1.262481e-07  0.001804  0.029864  0.018258  0.010173  0.001031  \n",
       "4  0.011383  1.262481e-07  0.001804  0.029864  0.018258  0.010173  0.001031  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_header = sfp.decode_Y(sgd.classes_).tolist()\n",
    "df_submission = pd.DataFrame(y_predictions_submission)\n",
    "display(df_submission.head())\n",
    "filename = \"model5_sgd.csv\"\n",
    "\n",
    "# uncomment the line below to generate a submission csv\n",
    "# df_submission.to_csv(filename, index=True, index_label=\"Id\", header=submission_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884262, 39)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.shape # should have 884262 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model received a log loss score of 2.70463 (rank 857 on the kaggle leaderboards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplemental notes on how we're preparing submission for kaggle\n",
    "\n",
    "* a header that describes the different categories... \n",
    "* id column (can be done with the to_csv function)\n",
    "* each row is a 39-element vector (probablity for each of the 39 classes for each incident)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we make the header from our predict_proba output?\n",
    "\n",
    "According to the scikit learn documentation, the output is: \"The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_\" [Also, thanks stackoverflow for an example.](http://stackoverflow.com/questions/16858652/how-to-find-the-corresponding-class-in-clf-predict-proba/16859091#16859091)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.28785352e-04   1.46040245e-02   4.21390038e-05   2.32175657e-04\n",
      "   6.09886256e-03   7.68048754e-04   4.61014967e-04   5.32388641e-03\n",
      "   9.57191183e-04   2.06098993e-04   2.03895235e-05   2.26943782e-04\n",
      "   3.77507657e-04   7.23371615e-03   6.86483484e-05   2.78060847e-03\n",
      "   2.34414787e-01   1.06819353e-03   1.75980393e-04   1.77654071e-02\n",
      "   2.93368280e-01   2.93238014e-01   1.68500963e-06   1.16694088e-03\n",
      "   4.35295021e-03   7.87012901e-03   7.25954355e-03   2.40792066e-02\n",
      "   6.65083371e-04   1.77824546e-04   8.86120636e-04   1.01969204e-03\n",
      "   1.27603785e-02   1.29439538e-07   2.47117958e-03   2.66008754e-02\n",
      "   1.25806702e-02   1.68402637e-02   1.50662397e-03]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38]\n",
      "['ARSON' 'ASSAULT' 'BAD CHECKS' 'BRIBERY' 'BURGLARY' 'DISORDERLY CONDUCT'\n",
      " 'DRIVING UNDER THE INFLUENCE' 'DRUG/NARCOTIC' 'DRUNKENNESS' 'EMBEZZLEMENT'\n",
      " 'EXTORTION' 'FAMILY OFFENSES' 'FORGERY/COUNTERFEITING' 'FRAUD' 'GAMBLING'\n",
      " 'KIDNAPPING' 'LARCENY/THEFT' 'LIQUOR LAWS' 'LOITERING' 'MISSING PERSON'\n",
      " 'NON-CRIMINAL' 'OTHER OFFENSES' 'PORNOGRAPHY/OBSCENE MAT' 'PROSTITUTION'\n",
      " 'RECOVERED VEHICLE' 'ROBBERY' 'RUNAWAY' 'SECONDARY CODES'\n",
      " 'SEX OFFENSES FORCIBLE' 'SEX OFFENSES NON FORCIBLE' 'STOLEN PROPERTY'\n",
      " 'SUICIDE' 'SUSPICIOUS OCC' 'TREA' 'TRESPASS' 'VANDALISM' 'VEHICLE THEFT'\n",
      " 'WARRANTS' 'WEAPON LAWS']\n"
     ]
    }
   ],
   "source": [
    "print(y_predictions_submission[0]) \n",
    "print(sgd.classes_) \n",
    "print(sfp.decode_Y(sgd.classes_)) # human readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then prototyped some code to prepare the submission csv. Uncomment if you want to investigate further. (It's currently commented out to prevent the creation of a csv by accident.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making a test_submission \n",
    "# subset = y_predictions[0:10]\n",
    "# df = pd.DataFrame(subset)\n",
    "# submission_header = sfp.decode_Y(sgd.classes_).tolist()\n",
    "# csv = df.to_csv(\"test_submission.csv\", index=True, index_label=\"Id\", header=submission_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the outputs human readable\n",
    "\n",
    "Let's print out the top 5 probabilities for a single incident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NON-CRIMINAL', 0.33951809665151417),\n",
      " ('OTHER OFFENSES', 0.25720351526986651),\n",
      " ('LARCENY/THEFT', 0.2501089870273529),\n",
      " ('WARRANTS', 0.031220511488807327),\n",
      " ('DRUG/NARCOTIC', 0.028424680228153726)]\n"
     ]
    }
   ],
   "source": [
    "a = y_predictions[0]\n",
    "\n",
    "# # strategy from http://stackoverflow.com/questions/6910641/how-to-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "ind = np.argpartition(a, -5)[-5:]\n",
    "top5_words = sfp.decode_Y(ind)\n",
    "top5 = {}\n",
    "for i in range(len(ind)):\n",
    "    top5[top5_words[i]] = a[ind[i]] \n",
    "\n",
    "import operator\n",
    "sorted_top5 = sorted(top5.items(), key=operator.itemgetter(1), reverse=True)\n",
    "pp.pprint(sorted_top5)\n",
    "\n",
    "# Visualize for a given incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: plot category vs. probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining model mistakes\n",
    "\n",
    "Let's try to find out more about what the model does well and what it doesn't do well. We have a multi class log loss value that tells us about how the model is doing on the 39-class classification problem of SF crime. One thing we could do to learn more about how it's doing on each class is computing the per-class log loss for the 39 crime categories.\n",
    "\n",
    "### Per-Class Log Loss\n",
    "\n",
    "We have a suspicion that our model is doing better on the classes which have more incidents. i.e. LARCENY/THEFT has many thousands more data points than ASSAULT. To investigate which classes our current multi-class model does poorly on, we are computing the per-class log loss for each class. \n",
    "\n",
    "In order to make our model's output compatible for per class log loss calculatuions, we have to do some processing of the y vectors into 1s and 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_per_class_log_loss(y_predictions, y_test):\n",
    "    # returns a 39-element list of the per_class_log_loss \n",
    "    # (one value per class. the index of the element corresponds with the class numbebr)\n",
    "    per_class_log_loss = []\n",
    "    for c in range(0, 39):\n",
    "        current_class = y_predictions[:, c]\n",
    "        not_current_class = 1 - y_predictions[:, c]\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "                \"truth\" : y_test,\n",
    "                \"current_class\" : current_class,\n",
    "                \"not_current_class\" : not_current_class\n",
    "            })\n",
    "\n",
    "        df.loc[df[\"truth\"] == c, \"binary_truth\"] = 1\n",
    "        df.loc[df[\"truth\"] != c, \"binary_truth\"] = 0\n",
    "\n",
    "        y_binary_truth = np.array(df[\"binary_truth\"])\n",
    "        y_binary_predictions = np.array(pd.concat([df[\"current_class\"], df[\"not_current_class\"]], axis=1))\n",
    "\n",
    "        current_class_log_loss = log_loss(y_binary_truth, y_binary_predictions)\n",
    "        per_class_log_loss.append(current_class_log_loss)\n",
    "    return per_class_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.5607579359377706,\n",
       " 3.405042232628908,\n",
       " 8.539750245870108,\n",
       " 8.509554987801085,\n",
       " 4.0437145149180642,\n",
       " 5.8886073873953029,\n",
       " 6.8050936294835109,\n",
       " 4.1047642729001943,\n",
       " 6.1076659346902273,\n",
       " 7.1081642928277198,\n",
       " 9.3600728987596558,\n",
       " 8.0499340685926839,\n",
       " 5.7035308354015744,\n",
       " 3.7883200238790771,\n",
       " 9.1519751090694719,\n",
       " 5.9509647769968614,\n",
       " 1.2537068177908739,\n",
       " 6.4417784148050465,\n",
       " 7.4812228538086778,\n",
       " 4.0815169690662358,\n",
       " 1.3326634554904344,\n",
       " 1.4040616480330577,\n",
       " 11.303716700854578,\n",
       " 5.3805656067111611,\n",
       " 5.7618638887726092,\n",
       " 4.2422167946261977,\n",
       " 5.6989205089301791,\n",
       " 4.3497523447100832,\n",
       " 6.2129717778883133,\n",
       " 8.3155141474097167,\n",
       " 6.1517761243905147,\n",
       " 7.2006868119994323,\n",
       " 3.7714753137602806,\n",
       " 12.87488456668437,\n",
       " 5.2022416903473321,\n",
       " 3.449296678594798,\n",
       " 3.9887628616280413,\n",
       " 3.4134463738859333,\n",
       " 6.0974083559844603]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "per_class_log_loss = compute_per_class_log_loss(y_predictions, y_test)\n",
    "display(per_class_log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these values on a scatter plot for a quick overview of the 39 values for per class log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAESCAYAAAD9gqKNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAuC/AKBlmx4sbz2yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdb450aad10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(0,39), per_class_log_loss)\n",
    "plt.xlabel(\"category #\")\n",
    "plt.ylabel(\"per_class_log_loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our NB model is worst at predicting class 33 (TREA) and 22 (PORNOGRAHY/OBSCENE MAT) and is the best at predicting the most common classes 16 (LARCENY/THEFT) and 1 (ASSAULT). Classes 33 and 22 also happen to be the categories with the least amount of incidents in the training dataset. \n",
    "\n",
    "The next steps would be to explore how these classes are different from the other classes, and find different ways to represent these data points to make better predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also find summary statistics for the per-class log loss (what's the average log loss value? etc).\n",
    "\n",
    "log loss has been a tough error metric to grasp conceptually... I'm a bit shaky on what it means to have a high or low log loss score, comparing log loss scores to each other, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 17, 2016\n",
    "\n",
    "#### Returning a sorted list of per-class log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes with the largest perclass logloss values:\n",
      "[['TREA', 'CLASS 33', 12.87488456668437],\n",
      " ['PORNOGRAPHY/OBSCENE MAT', 'CLASS 22', 11.303716700854578],\n",
      " ['EXTORTION', 'CLASS 10', 9.3600728987596558],\n",
      " ['GAMBLING', 'CLASS 14', 9.1519751090694719],\n",
      " ['BAD CHECKS', 'CLASS 2', 8.539750245870108]]\n",
      "\n",
      "classes with the lowest perclass logloss values:\n",
      "[['LARCENY/THEFT', 'CLASS 16', 1.2537068177908739],\n",
      " ['NON-CRIMINAL', 'CLASS 20', 1.3326634554904344],\n",
      " ['OTHER OFFENSES', 'CLASS 21', 1.4040616480330577],\n",
      " ['ASSAULT', 'CLASS 1', 3.405042232628908],\n",
      " ['WARRANTS', 'CLASS 37', 3.4134463738859333]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sorted_perclass_logloss(labelstrs, log_losses):\n",
    "    spl = []\n",
    "    for i in range(len(labelstrs)):\n",
    "        spl.append([labels[i], \"CLASS \" + str(i), log_losses[i]])\n",
    "    spl = sorted(spl, key=lambda x: x[2]) # sort by the class' log loss value (the 2nd element in the list within)\n",
    "    return spl\n",
    "\n",
    "def n_highest_perclass_logloss(spl, n=39):\n",
    "    # if no n is specified, this function returns the entire list sorted from highest to lowest perclass logloss\n",
    "    reversed_spl = list(reversed(spl))\n",
    "    return reversed_spl[0:n]\n",
    "\n",
    "def n_lowest_perclass_logloss(spl, n=39):\n",
    "    # if no n is specified, this function returns the entire list sorted from lowest to highest perclass logloss\n",
    "    # spl is already in order from lowest to highest perclass logloss value\n",
    "    return spl[0:n]\n",
    "\n",
    "spl = sorted_perclass_logloss(labels, per_class_log_loss)\n",
    "print \"classes with the largest perclass logloss values:\"\n",
    "pp.pprint(n_highest_perclass_logloss(spl, 5))\n",
    "print\n",
    "print \"classes with the lowest perclass logloss values:\"\n",
    "pp.pprint(n_lowest_perclass_logloss(spl, 5))\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Next Steps\n",
    "\n",
    "#### Visualizing the performance of our models\n",
    "\n",
    "We could...\n",
    "* Plot the log loss values of different classifiers\n",
    "* Compare training performance vs. kaggle performance\n",
    "* Compare classifiers with different hyperparameter values used\n",
    "\n",
    "#### Running several trials and summary statistics for those trials\n",
    "\n",
    "#### Comparing different implementations (change in performance vs. frequency) \n",
    "\n",
    "#### Doing some more feature engineering; comparing the performance of the model with different feature combinations\n",
    "\n",
    "#### Another submission to kaggle!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
