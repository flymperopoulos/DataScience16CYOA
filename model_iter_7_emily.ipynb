{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emily Wang and Filippos Lymperopoulos | Data Science 2016 | CYOA: sfcrime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 17, 2016\n",
    "\n",
    "### Goals for *model_iter_7_emily.ipynb*\n",
    "\n",
    "* Improve readability\n",
    "* Several different folds; summary statistics on k folds\n",
    "* Different ideas for feature engineering/encoding\n",
    "\n",
    "Note: Apologies if model_iter_7_emily.ipynb is a misleading title; the models themselves are similar to the models that have been happening since notebook 4 or 5, and have been using the same preprocessor class (SFP). What's different is the accompanying analysis and additional commentary from the previous model_iter_X notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 15, 2016\n",
    "\n",
    "### The process\n",
    "\n",
    "* Import libraries and training data\n",
    "* Feature engineering / preprocessing: \n",
    "    * Make \"useful\" combinations of features to give our model; \n",
    "    * Also encode categorical things in an intelligent way; \n",
    "    * Can choose to only use a subset of features if desired\n",
    "* Partition your data (cross-validation kfolds, etc)\n",
    "* Model fit\n",
    "* Make some predictions\n",
    "* Compute the logloss score\n",
    "* Reflect; iterate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's import some useful libraries and import the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/greenteawarrior/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>WARRANT ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-13 23:33:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>VANNESS AV / GREENWICH ST</td>\n",
       "      <td>-122.424363</td>\n",
       "      <td>37.800414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dates        Category                  Descript  DayOfWeek  \\\n",
       "0 2015-05-13 23:53:00        WARRANTS            WARRANT ARREST  Wednesday   \n",
       "1 2015-05-13 23:53:00  OTHER OFFENSES  TRAFFIC VIOLATION ARREST  Wednesday   \n",
       "2 2015-05-13 23:33:00  OTHER OFFENSES  TRAFFIC VIOLATION ARREST  Wednesday   \n",
       "\n",
       "  PdDistrict      Resolution                    Address           X          Y  \n",
       "0   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST -122.425892  37.774599  \n",
       "1   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST -122.425892  37.774599  \n",
       "2   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST -122.424363  37.800414  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "\n",
    "# Convert the Dates column of our provided data from string to datetime format.\n",
    "train = pd.read_csv('train.csv', parse_dates = ['Dates'])\n",
    "test = pd.read_csv('test.csv', parse_dates = ['Dates'])\n",
    "\n",
    "# Print the first 3 rows of the dataframe.\n",
    "display(train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering, Preprocessing\n",
    "\n",
    "Make a class that will:\n",
    "* Extract the time features we want to use for the model (e.g. year, season, month, day, etc.)\n",
    "* Encode categorical variables in a meaningful way: contains a preprocessor that can both transform and inverse_transform the categorical variables\n",
    "* Return a transformed dataframe to be given to the model\n",
    "* Maybe: allow for some flexibility with what is in the transformed dataframe (to iterate quickly) (e.g. choosing how many time features you want in this experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SFP = SFCrime Preprocessor\n",
    "class SFP():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.Y_encoder = preprocessing.LabelEncoder()\n",
    "    \n",
    "    # Prepare inputs\n",
    "    def prep_district(self):\n",
    "        # one hot encoding\n",
    "        return pd.get_dummies(self.data.PdDistrict)\n",
    "    \n",
    "    def prep_hour(self):\n",
    "        # a continuous value from 0 to 23\n",
    "        return self.data.Dates.dt.hour # Gets the hour portion form the \"Dates\" column\n",
    "    \n",
    "    def prep_day(self):\n",
    "        # one hot encoding\n",
    "        return pd.get_dummies(self.data.DayOfWeek)\n",
    "    \n",
    "    def prep_years(self):\n",
    "        # beware: 2015 has significantly less incidents than the other years in this dataset.        \n",
    "        pass\n",
    "    \n",
    "    def concat_features(self):\n",
    "        hour = self.prep_hour()\n",
    "        day = self.prep_day()\n",
    "        district = self.prep_district()\n",
    "        return pd.concat([hour, day, district], axis=1)\n",
    "    \n",
    "    # Encode or decode classes\n",
    "    def encode_Y(self, Y):\n",
    "        return self.Y_encoder.fit_transform(Y)\n",
    "\n",
    "    def decode_Y(self, encoded_Y):\n",
    "        return self.Y_encoder.inverse_transform(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sfp = SFP(train)\n",
    "X = sfp.concat_features()\n",
    "y = sfp.encode_Y(train.Category)\n",
    "X.rename(columns = {'Dates':'Hour'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check\n",
    "\n",
    "Print out the head for X to ensure that our preprocessing via the SFP class worked as expected. \n",
    "We should have a number between 0 and 39 for Hour. Each incident should have a 1 in one of the weekday columns and 0 in all other weekday columns. Each incident should have a 1 in one of the PdDistrict columns and 0 ina ll other PdDistrict columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>BAYVIEW</th>\n",
       "      <th>CENTRAL</th>\n",
       "      <th>INGLESIDE</th>\n",
       "      <th>MISSION</th>\n",
       "      <th>NORTHERN</th>\n",
       "      <th>PARK</th>\n",
       "      <th>RICHMOND</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>TARAVAL</th>\n",
       "      <th>TENDERLOIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hour  Friday  Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  \\\n",
       "0    23       0       0         0       0         0        0          1   \n",
       "1    23       0       0         0       0         0        0          1   \n",
       "2    23       0       0         0       0         0        0          1   \n",
       "3    23       0       0         0       0         0        0          1   \n",
       "4    23       0       0         0       0         0        0          1   \n",
       "\n",
       "   BAYVIEW  CENTRAL  INGLESIDE  MISSION  NORTHERN  PARK  RICHMOND  SOUTHERN  \\\n",
       "0        0        0          0        0         1     0         0         0   \n",
       "1        0        0          0        0         1     0         0         0   \n",
       "2        0        0          0        0         1     0         0         0   \n",
       "3        0        0          0        0         1     0         0         0   \n",
       "4        0        0          0        0         0     1         0         0   \n",
       "\n",
       "   TARAVAL  TENDERLOIN  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note: this X and Y in particular are from the data in train.csv. See previous section for details.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the training data to the algorithm\n",
    "\n",
    "Decision trees are known to be good at handling categorical data. Let's try using some of the decision tree variations in scikit learn (decision tree, random forest, gradient boost, etc) and tweak some hyperparameters. We might even do some ensemble learning. Oooh shiny!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "_ = dtc.fit(X_train, y_train)\n",
    "y_predictions = dtc.predict_proba(X_test)\n",
    "dtc_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.007115993440352"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_log_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filippos says he thinks this log loss value of 3.007115993440352 is very deece. Confirmed by looking at the kaggle leaderboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A baseline: Predict all unseen incidents as LARCENY/THEFT\n",
    "\n",
    "So... we know from the exploration phase that the dataset is not evenly distributed (i.e. not all 39 classes are equally represented in the dataset. What's the log loss for a model that just predicts that all new data is the most common crime type ('LARCENCY/THEFT') ? This means that the baseline model outputs the same prediction each time: a 39 dimensional vector of 0s and a 1 in the corresponding column for LARCENY/THEFT. The 1 signifies 100% confidence that the unseen incident is LARCENY/THEFT. \n",
    "\n",
    "Another idea for a baseline would be to randomly guess an intger between 0 and 38 for each unseen incident. \n",
    "\n",
    "Since log loss is a new error metric for us amd we don't already have an intuition for what log loss values are possible/reasonable for this project, this baseline provides a sanity check to ensure that we should be getting a better score on the models we are trying to train well with feature engineering, different classifiers, ensembling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded = sfp.decode_Y(dtc.classes_)\n",
    "\n",
    "for x in range(len(decoded)):\n",
    "    if decoded[x] == \"LARCENY/THEFT\": \n",
    "        # we know from the exploration phase that LARCENY/THEFT was the most popular crime in train.csv\n",
    "        popular_index = x\n",
    "\n",
    "baseline_y_pred = np.zeros((y_test.shape[0], 39))\n",
    "baseline_y_pred[:,16] = 1\n",
    "\n",
    "baseline_log_loss = log_loss(y_test, baseline_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.658162800015461"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Using some hyperparameter values from DataQuest mission 75\n",
    "rf = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=4, min_samples_leaf=1) \n",
    "_ = rf.fit(X_train, y_train)\n",
    "y_predictions = rf.predict_proba(X_test)\n",
    "rf_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0111558103212417"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "[scikit learn cheatsheet advises us to look into SGD classifiers!](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "\n",
    "Understanding SGD:\n",
    "* yay Andrew Ng ML video\n",
    "* batch gradient descent (looks at all of the training examples in every iteration)\n",
    "* stochastic gradient descent (looks at only one training example in every iteration)\n",
    "    * how well is my hypothesis doing on a single example? for a given theta and x,y pair\n",
    "* different in the implementation details and making progress towards the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "_ = sgd.fit(X_train, y_train)\n",
    "y_predictions = sgd.predict_proba(X_test)\n",
    "sgd_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8531127014796498"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ooooooooooohhhh\"  -- Emily and Filippos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb = BernoulliNB()\n",
    "_ = nb.fit(X_train, y_train)\n",
    "y_nb_predictions = nb.predict_proba(X_test)\n",
    "nb_log_loss = log_loss(y_test, y_nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.609770164645326"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ooooooooooohhhh\" (again)  -- Emily and Filippos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps\n",
    "\n",
    "ASAP:\n",
    "* ~~prepare a submission to kaggle~~\n",
    "* Visualization of performance of the different models (include an ensemble learning result in here too)\n",
    "* ~~Comparison of log loss to the \"predict the most common thing\" baseline model (predict all unseen incidents to be LARCENY/THEFT)~~\n",
    "* *10 fold validation + mean and standard deviation for the metrics* - Emily\n",
    "* ~~log loss on each separate class (turn the multi class problem into 39 binary problems)~~\n",
    "    * --> discover more specifically which things the model predicts well and not so well \n",
    "    * --> more exploration and feature engineering in hopes of resolving the difference in performance between crime classes\n",
    "\n",
    "\n",
    "Backlog:\n",
    "* ~~explanation on SGD black box~~\n",
    "* ~~How to translate the 39-element outputs into more \"human readable\" outputs: TOP5~~\n",
    "* *Creative approaches to ensemble learning!* - Filippos \n",
    "* ~~Naive Bayes~~\n",
    "* Try playing with hyperparameters; see how changes in those values impact the logloss, and plot them (hyperparameter value vs. log loss)\n",
    "\n",
    "Process comments:\n",
    "* Feb 15, 2016: We're relatively happy with our current preprocessor to pause on the feature engineering and do experiments with the predictive models; we'll cycle back to the feature engineering if there's time and interest. :)\n",
    "* Feb 17, 2016: Now that we have the per class log loss values, we have some motivation to go back to the exploration phase to better understand the classes which the model had more trouble with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 16, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it out on the test set\n",
    "\n",
    "Preparing our inputs (the X matrix): This should all look very familiar to the cells in the previous sections.\n",
    "\n",
    "Be careful of your variable names!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>BAYVIEW</th>\n",
       "      <th>CENTRAL</th>\n",
       "      <th>INGLESIDE</th>\n",
       "      <th>MISSION</th>\n",
       "      <th>NORTHERN</th>\n",
       "      <th>PARK</th>\n",
       "      <th>RICHMOND</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>TARAVAL</th>\n",
       "      <th>TENDERLOIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hour  Friday  Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  \\\n",
       "0    23       0       0         0       0         0        0          1   \n",
       "1    23       0       0         0       0         0        0          1   \n",
       "2    23       0       0         0       0         0        0          1   \n",
       "3    23       0       0         0       0         0        0          1   \n",
       "4    23       0       0         0       0         0        0          1   \n",
       "\n",
       "   BAYVIEW  CENTRAL  INGLESIDE  MISSION  NORTHERN  PARK  RICHMOND  SOUTHERN  \\\n",
       "0        0        0          0        0         1     0         0         0   \n",
       "1        0        0          0        0         1     0         0         0   \n",
       "2        0        0          0        0         1     0         0         0   \n",
       "3        0        0          0        0         1     0         0         0   \n",
       "4        0        0          0        0         0     1         0         0   \n",
       "\n",
       "   TARAVAL  TENDERLOIN  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>BAYVIEW</th>\n",
       "      <th>CENTRAL</th>\n",
       "      <th>INGLESIDE</th>\n",
       "      <th>MISSION</th>\n",
       "      <th>NORTHERN</th>\n",
       "      <th>PARK</th>\n",
       "      <th>RICHMOND</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>TARAVAL</th>\n",
       "      <th>TENDERLOIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dates  Friday  Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  \\\n",
       "0     23       0       0         0       1         0        0          0   \n",
       "1     23       0       0         0       1         0        0          0   \n",
       "2     23       0       0         0       1         0        0          0   \n",
       "3     23       0       0         0       1         0        0          0   \n",
       "4     23       0       0         0       1         0        0          0   \n",
       "\n",
       "   BAYVIEW  CENTRAL  INGLESIDE  MISSION  NORTHERN  PARK  RICHMOND  SOUTHERN  \\\n",
       "0        1        0          0        0         0     0         0         0   \n",
       "1        1        0          0        0         0     0         0         0   \n",
       "2        0        0          0        0         1     0         0         0   \n",
       "3        0        0          1        0         0     0         0         0   \n",
       "4        0        0          1        0         0     0         0         0   \n",
       "\n",
       "   TARAVAL  TENDERLOIN  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sfp_submission will be used to preprocess the data from test.csv\n",
    "sfp_submission = SFP(test) \n",
    "X_submission = sfp_submission.concat_features() \n",
    "\n",
    "# Sanity check: These should not be the same, because X is from train.csv and X_submission is from test.csv\n",
    "display(X.head())\n",
    "display(X_submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As a first pass, using the sgd we trained earlier in the notebook\n",
    "y_predictions_submission = sgd.predict_proba(X_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.064264</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.041654</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.049399</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.107907</td>\n",
       "      <td>3.135641e-07</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.333059</td>\n",
       "      <td>0.054069</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.012087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.064264</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.041654</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.049399</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.107907</td>\n",
       "      <td>3.135641e-07</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.333059</td>\n",
       "      <td>0.054069</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.012087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.045954</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.058756</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.045302</td>\n",
       "      <td>0.005993</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.093017</td>\n",
       "      <td>3.462607e-07</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.327207</td>\n",
       "      <td>0.038411</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>0.005303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.059057</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.037836</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.028112</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.101287</td>\n",
       "      <td>3.093449e-07</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.350008</td>\n",
       "      <td>0.072921</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.009071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.059057</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.037836</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.028112</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.101287</td>\n",
       "      <td>3.093449e-07</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.350008</td>\n",
       "      <td>0.072921</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.009071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.002906  0.064264  0.000062  0.000080  0.041654  0.000633  0.000526   \n",
       "1  0.002906  0.064264  0.000062  0.000080  0.041654  0.000633  0.000526   \n",
       "2  0.001258  0.045954  0.000073  0.000059  0.058756  0.000929  0.000628   \n",
       "3  0.001608  0.059057  0.000064  0.000071  0.037836  0.000566  0.000565   \n",
       "4  0.001608  0.059057  0.000064  0.000071  0.037836  0.000566  0.000565   \n",
       "\n",
       "         7         8         9     ...           29        30        31  \\\n",
       "0  0.049399  0.004394  0.000352    ...     0.000429  0.001602  0.000230   \n",
       "1  0.049399  0.004394  0.000352    ...     0.000429  0.001602  0.000230   \n",
       "2  0.045302  0.005993  0.000442    ...     0.000406  0.002548  0.000267   \n",
       "3  0.028112  0.004031  0.000368    ...     0.000441  0.001717  0.000248   \n",
       "4  0.028112  0.004031  0.000368    ...     0.000441  0.001717  0.000248   \n",
       "\n",
       "         32            33        34        35        36        37        38  \n",
       "0  0.107907  3.135641e-07  0.001912  0.333059  0.054069  0.006596  0.012087  \n",
       "1  0.107907  3.135641e-07  0.001912  0.333059  0.054069  0.006596  0.012087  \n",
       "2  0.093017  3.462607e-07  0.002006  0.327207  0.038411  0.006782  0.005303  \n",
       "3  0.101287  3.093449e-07  0.001427  0.350008  0.072921  0.004571  0.009071  \n",
       "4  0.101287  3.093449e-07  0.001427  0.350008  0.072921  0.004571  0.009071  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_header = sfp.decode_Y(sgd.classes_).tolist()\n",
    "df_submission = pd.DataFrame(y_predictions_submission)\n",
    "display(df_submission.head())\n",
    "filename = \"model5_sgd.csv\"\n",
    "\n",
    "# uncomment the line below to generate a submission csv\n",
    "# df_submission.to_csv(filename, index=True, index_label=\"Id\", header=submission_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884262, 39)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.shape # should have 884262 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model received a log loss score of 2.70463 (rank 857 on the kaggle leaderboards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplemental notes on how we're preparing submission for kaggle\n",
    "\n",
    "* a header that describes the different categories... \n",
    "* id column (can be done with the to_csv function)\n",
    "* each row is a 39-element vector (probablity for each of the 39 classes for each incident)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we make the header from our predict_proba output?\n",
    "\n",
    "According to the scikit learn documentation, the output is: \"The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_\" [Also, thanks stackoverflow for an example.](http://stackoverflow.com/questions/16858652/how-to-find-the-corresponding-class-in-clf-predict-proba/16859091#16859091)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.90613883e-03   6.42635212e-02   6.18319332e-05   8.02283801e-05\n",
      "   4.16535260e-02   6.32875860e-04   5.25851703e-04   4.93989588e-02\n",
      "   4.39397026e-03   3.52448777e-04   8.49675270e-05   5.60892454e-04\n",
      "   9.66821768e-04   1.81211629e-03   3.93099594e-05   5.35208817e-03\n",
      "   1.97186142e-02   4.03447253e-04   6.17424465e-05   1.14273050e-02\n",
      "   3.72122302e-02   7.65151915e-02   3.45474263e-06   5.20150662e-03\n",
      "   1.75936619e-03   1.00398155e-01   1.82289547e-03   5.38210867e-02\n",
      "   6.78728908e-04   4.28686496e-04   1.60228732e-03   2.29723572e-04\n",
      "   1.07906656e-01   3.13564135e-07   1.91191448e-03   3.33058691e-01\n",
      "   5.40693016e-02   6.59601940e-03   1.20871341e-02]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38]\n",
      "['ARSON' 'ASSAULT' 'BAD CHECKS' 'BRIBERY' 'BURGLARY' 'DISORDERLY CONDUCT'\n",
      " 'DRIVING UNDER THE INFLUENCE' 'DRUG/NARCOTIC' 'DRUNKENNESS' 'EMBEZZLEMENT'\n",
      " 'EXTORTION' 'FAMILY OFFENSES' 'FORGERY/COUNTERFEITING' 'FRAUD' 'GAMBLING'\n",
      " 'KIDNAPPING' 'LARCENY/THEFT' 'LIQUOR LAWS' 'LOITERING' 'MISSING PERSON'\n",
      " 'NON-CRIMINAL' 'OTHER OFFENSES' 'PORNOGRAPHY/OBSCENE MAT' 'PROSTITUTION'\n",
      " 'RECOVERED VEHICLE' 'ROBBERY' 'RUNAWAY' 'SECONDARY CODES'\n",
      " 'SEX OFFENSES FORCIBLE' 'SEX OFFENSES NON FORCIBLE' 'STOLEN PROPERTY'\n",
      " 'SUICIDE' 'SUSPICIOUS OCC' 'TREA' 'TRESPASS' 'VANDALISM' 'VEHICLE THEFT'\n",
      " 'WARRANTS' 'WEAPON LAWS']\n"
     ]
    }
   ],
   "source": [
    "print(y_predictions_submission[0]) \n",
    "print(sgd.classes_) \n",
    "print(sfp.decode_Y(sgd.classes_)) # human readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then prototyped some code to prepare the submission csv. Uncomment if you want to investigate further. (It's currently commented out to prevent the creation of a csv by accident.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making a test_submission \n",
    "# subset = y_predictions[0:10]\n",
    "# df = pd.DataFrame(subset)\n",
    "# submission_header = sfp.decode_Y(sgd.classes_).tolist()\n",
    "# csv = df.to_csv(\"test_submission.csv\", index=True, index_label=\"Id\", header=submission_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the outputs human readable\n",
    "\n",
    "Let's print out the top 5 probabilities for a single incident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DRUG/NARCOTIC', 0.25548631869345589),\n",
      " ('VANDALISM', 0.17038493560188622),\n",
      " ('ROBBERY', 0.1086009775353567),\n",
      " ('SUSPICIOUS OCC', 0.089566185825134689),\n",
      " ('OTHER OFFENSES', 0.075685209915902898)]\n"
     ]
    }
   ],
   "source": [
    "a = y_predictions[0]\n",
    "\n",
    "# # strategy from http://stackoverflow.com/questions/6910641/how-to-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "ind = np.argpartition(a, -5)[-5:]\n",
    "top5_words = sfp.decode_Y(ind)\n",
    "top5 = {}\n",
    "for i in range(len(ind)):\n",
    "    top5[top5_words[i]] = a[ind[i]] \n",
    "\n",
    "import operator\n",
    "sorted_top5 = sorted(top5.items(), key=operator.itemgetter(1), reverse=True)\n",
    "pp.pprint(sorted_top5)\n",
    "\n",
    "# Visualize for a given incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: plot category vs. probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining model mistakes\n",
    "\n",
    "Let's try to find out more about what the model does well and what it doesn't do well. We have a multi class log loss value that tells us about how the model is doing on the 39-class classification problem of SF crime. One thing we could do to learn more about how it's doing on each class is computing the per-class log loss for the 39 crime categories.\n",
    "\n",
    "### Per-Class Log Loss\n",
    "\n",
    "We have a suspicion that our model is doing better on the classes which have more incidents. i.e. LARCENY/THEFT has many thousands more data points than ASSAULT. To investigate which classes our current multi-class model does poorly on, we are computing the per-class log loss for each class. \n",
    "\n",
    "In order to make our model's output compatible for per class log loss calculatuions, we have to do some processing of the y vectors into 1s and 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_per_class_log_loss(y_predictions, y_test):\n",
    "    # returns a 39-element list of the per_class_log_loss \n",
    "    # (one value per class. the index of the element corresponds with the class numbebr)\n",
    "    per_class_log_loss = []\n",
    "    for c in range(0, 39):\n",
    "        current_class = y_predictions[:, c]\n",
    "        not_current_class = 1 - y_predictions[:, c]\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "                \"truth\" : y_test,\n",
    "                \"current_class\" : current_class,\n",
    "                \"not_current_class\" : not_current_class\n",
    "            })\n",
    "\n",
    "        df.loc[df[\"truth\"] == c, \"binary_truth\"] = 1\n",
    "        df.loc[df[\"truth\"] != c, \"binary_truth\"] = 0\n",
    "\n",
    "        y_binary_truth = np.array(df[\"binary_truth\"])\n",
    "        y_binary_predictions = np.array(pd.concat([df[\"current_class\"], df[\"not_current_class\"]], axis=1))\n",
    "\n",
    "        current_class_log_loss = log_loss(y_binary_truth, y_binary_predictions)\n",
    "        per_class_log_loss.append(current_class_log_loss)\n",
    "    return per_class_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.1576242963216803,\n",
       " 2.5006400722514561,\n",
       " 8.1929140713607875,\n",
       " 8.8051817188632437,\n",
       " 2.8856682827174494,\n",
       " 5.8536790785561523,\n",
       " 6.5761272986505492,\n",
       " 2.7434442991584227,\n",
       " 5.0674374732432188,\n",
       " 6.6485019178993543,\n",
       " 8.3705767249186085,\n",
       " 7.3640956740807253,\n",
       " 5.0599434110492476,\n",
       " 4.500291056998968,\n",
       " 9.2535273206270947,\n",
       " 5.4843760115746845,\n",
       " 2.1430789733502462,\n",
       " 6.8735035342944339,\n",
       " 7.861954152622971,\n",
       " 4.1307277278735119,\n",
       " 2.2395984226592605,\n",
       " 1.9485743360831171,\n",
       " 10.726095834648927,\n",
       " 4.4363591495438746,\n",
       " 6.127760341461169,\n",
       " 2.6745861739836596,\n",
       " 6.3942246827320588,\n",
       " 3.7378916419036203,\n",
       " 6.06428183388463,\n",
       " 7.6638621704473922,\n",
       " 5.6297666647683657,\n",
       " 7.7594383658940878,\n",
       " 2.4440617959607955,\n",
       " 12.163778183512649,\n",
       " 5.2248780162037534,\n",
       " 1.8221516678618621,\n",
       " 3.1548210053110823,\n",
       " 3.8715312562870525,\n",
       " 4.7318487920474164]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "per_class_log_loss = compute_per_class_log_loss(y_predictions, y_test)\n",
    "display(per_class_log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these values on a scatter plot for a quick overview of the 39 values for per class log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAFmCAYAAAB5pHO7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wEO+atxGaWdQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69ce0c0d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(0,39), per_class_log_loss)\n",
    "plt.xlabel(\"category #\")\n",
    "plt.ylabel(\"per_class_log_loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our NB model is worst at predicting class 33 (TREA) and 22 (PORNOGRAHY/OBSCENE MAT) and is the best at predicting the most common classes 16 (LARCENY/THEFT) and 1 (ASSAULT). Classes 33 and 22 also happen to be the categories with the least amount of incidents in the training dataset. \n",
    "\n",
    "The next steps would be to explore how these classes are different from the other classes, and find different ways to represent these data points to make better predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also find summary statistics for the per-class log loss (what's the average log loss value? etc).\n",
    "\n",
    "log loss has been a tough error metric to grasp conceptually... I'm a bit shaky on what it means to have a high or low log loss score, comparing log loss scores to each other, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 17, 2016\n",
    "\n",
    "#### Returning a sorted list of per-class log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sorted_perclass_logloss(labelstrs, log_losses):\n",
    "    spl = []\n",
    "    for i in range(len(labelstrs)):\n",
    "        spl.append([labels[i], \"CLASS \" + str(i), log_losses[i]])\n",
    "    spl = sorted(spl, key=lambda x: x[2]) # sort by the class' log loss value (the 2nd element in the list within)\n",
    "    return spl\n",
    "\n",
    "def n_highest_perclass_logloss(spl, n=39):\n",
    "    # if no n is specified, this function returns the entire list sorted from highest to lowest perclass logloss\n",
    "    reversed_spl = list(reversed(spl))\n",
    "    return reversed_spl[0:n]\n",
    "\n",
    "def n_lowest_perclass_logloss(spl, n=39):\n",
    "    # if no n is specified, this function returns the entire list sorted from lowest to highest perclass logloss\n",
    "    # spl is already in order from lowest to highest perclass logloss value\n",
    "    return spl[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = sfp.decode_Y(sgd.classes_)\n",
    "spl = sorted_perclass_logloss(labels, per_class_log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes with the largest perclass logloss values\n",
    "\n",
    "These are the classes which the model has been doing the (relative) worst on classifying correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['TREA', 'CLASS 33', 12.163778183512649],\n",
      " ['PORNOGRAPHY/OBSCENE MAT', 'CLASS 22', 10.726095834648927],\n",
      " ['GAMBLING', 'CLASS 14', 9.2535273206270947],\n",
      " ['BRIBERY', 'CLASS 3', 8.8051817188632437],\n",
      " ['EXTORTION', 'CLASS 10', 8.3705767249186085]]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(n_highest_perclass_logloss(spl, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes with the lowest perclass logloss values\n",
    "\n",
    "These are the classes which the model has been doing the (relative) best at classifying corectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['VANDALISM', 'CLASS 35', 1.8221516678618621],\n",
      " ['OTHER OFFENSES', 'CLASS 21', 1.9485743360831171],\n",
      " ['LARCENY/THEFT', 'CLASS 16', 2.1430789733502462],\n",
      " ['NON-CRIMINAL', 'CLASS 20', 2.2395984226592605],\n",
      " ['SUSPICIOUS OCC', 'CLASS 32', 2.4440617959607955]]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(n_lowest_perclass_logloss(spl, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross validation\n",
    "\n",
    "As an alternative validation method to the single test train split we did earlier in the notebook... As a first step I'd like to implement the k-fold cross-validation with a SGD classifier, make predictions on the data in test.csv, and submit to kaggle to compare the results from when we did the single test train split.\n",
    "\n",
    "Warning: This is going back to an earlier stage in the predictive modelling pipeline; please don't mix up the variables after this cell with the work done above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every fold, a model is trained using k-1 of the folds as training data. This model is then validated on the one fold that was not used for training. The log loss of the model is an average of the log losses for all the folds. This way we don't need to reserve a validation set and suffer the consequences of having datasets with relatively few incidents to train on.\n",
    "\n",
    "We can compare the cross validation scores between different classifiers (SGD, Random Forest, Decision Tree, Naive Bayes, etc) and get more metrics on how the models are performing. These metrics will then inform us to pick a model with which to fit the entire dataset (and then apply this model to the test.csv dataset).\n",
    "\n",
    "[The responses to this stackoverflow post](http://stackoverflow.com/questions/32700797/saving-a-cross-validation-trained-model-in-scikit) helped clarify what role cross validation has in the process (you do cross validation when trying out models and then train the classifier using the entire training dataset if it has nice results during cross validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k fold cross validation for the models we tried earlier in this notebook\n",
    "\n",
    "The intent here is for k fold cross validation to help us observe the effects of hyperparameters and compare performance between models trained with the same X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiating sfp, X, and y again\n",
    "sfp = SFP(train)\n",
    "X = sfp.concat_features()\n",
    "y = sfp.encode_Y(train.Category)\n",
    "X.rename(columns = {'Dates':'Hour'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# relevant cross validation parameters\n",
    "k = 5 # number of folds for k fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function for running cross_val_score and printing results\n",
    "def cvs(clf, desired_k):\n",
    "    # cvs is an abbreviation for cross validation score\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, scoring=\"log_loss\", cv=desired_k)\n",
    "    print(\"(k fold cross validation) log loss: %0.2f (+/- %0.2f)\" % (-1 * scores.mean(), scores.std() * 2))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasional issue with the classifiers that came up while prototyping this code with `cv = 10`: \n",
    "Sometimes the folds don't have incidents from all 39 classes; wondering if there's some way to guarantee this. I know that test_train_split has a some stratification parameter, but this doesn't seem to be the case with cross_val_score. :l\n",
    "\n",
    "To explain the `-1 * scores.mean()` in the cvs function's print statement: \n",
    "The log loss returned by cross_val_score is negative... this is a notation thing with sk learn. Apparently with the sk learn scoring API, the score thata is returned is negated when it is a score that should be minimized (this is the case with log loss) and left positive if it is a score that should be mximized. The actual log loss is the positive version of this number. [Thanks to this stackoverflow post for clarifying; this particular question is about MSE but the same issue came up with negative values being returned.](https://stackoverflow.com/questions/21443865/scikit-learn-cross-validation-negative-values-with-mean-squared-error)\n",
    "\n",
    "Note: \n",
    "The baseline model we made isn't included in the cells below; if desired we can try making a clf wrapper for it to feed into cross_val_score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(k fold cross validation) log loss: 2.95 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "cvs(clf, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(k fold cross validation) log loss: 2.95 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=4, min_samples_leaf=1) \n",
    "cvs(clf, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(k fold cross validation) log loss: 2.72 (+/- 0.10)\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "cvs(clf, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(k fold cross validation) log loss: 2.61 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "cvs(clf, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the trend in the k fold cross validation log loss values between these different classifiers matched the trend we saw with the simple 40/60 test train split. Naive Bayes is still in the lead, with SGD performing only slightly worse. The Decision Tree and Random Forest models had the same log loss value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps?\n",
    "\n",
    "#### Doing some more feature engineering; comparing the performance of the model with different feature combinations\n",
    "* Especially to help the classes with the highest perclass logloss values\n",
    "\n",
    "#### Visualizing the performance of our models\n",
    "\n",
    "Ideas! (some of these can build from the work done earlier in this notebook)\n",
    "* Plot the log loss values of different classifiers\n",
    "* Compare training performance vs. kaggle performance\n",
    "* Compare classifiers with different hyperparameter values used\n",
    "* Summary statistics\n",
    "* Making a confusion matrix. [See the example on the sk learn website.](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
